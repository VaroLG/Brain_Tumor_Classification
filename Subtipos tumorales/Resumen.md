#  Clasificaci贸n de Subtipos tumorales de cerebro con EfficientNetB1

Este proyecto implementa una red neuronal convolucional basada en **EfficientNetB1** para la **clasificaci贸n autom谩tica de subtipos tumorales de cerebro** a partir de im谩genes de resonancia magn茅tica (MRI) de un dataset de Kaggle.  

https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri

Se utiliza un enfoque de **transfer learning**, ajustando las capas superiores del modelo preentrenado e incorporando capas densas personalizadas con t茅cnicas de regularizaci贸n y normalizaci贸n.

---

## 锔 Descripci贸n del Proyecto

El objetivo es clasificar im谩genes de gliomas seg煤n su **subtipo tumoral** (Glioma, Meningioma, Tumor de la gl谩ndula hip贸fisis y ausencia de tumor), optimizando la precisi贸n del modelo mediante **entrenamiento por etapas** y estrategias de **regularizaci贸n** como *Dropout*.

El flujo general del proyecto incluye:
1. Carga y preprocesamiento de datos.
2. Construcci贸n y configuraci贸n de la red EfficientNetB1.
3. Entrenamiento del modelo en m煤ltiples etapas.
4. Evaluaci贸n mediante m茅tricas de rendimiento y visualizaci贸n de resultados.

---

## З Arquitectura del Modelo

El modelo se basa en la arquitectura **EfficientNetB1**, que optimiza simult谩neamente profundidad, ancho y resoluci贸n de la red para obtener un equilibrio entre **rendimiento y eficiencia computacional**.  

A partir de esta base preentrenada (en ImageNet), se a帽aden capas densas personalizadas para adaptar el modelo a la tarea de clasificaci贸n de subtipos de tumor.

###  Estructura general:

1. **Base convolucional (EfficientNetB1 preentrenada)**  
   - Se utiliza como *feature extractor*, congelando inicialmente sus pesos para aprovechar el conocimiento previo aprendido sobre patrones visuales.  
   - Incluye bloques MBConv (Mobile Inverted Bottleneck Convolution) con conexiones residuales, *batch normalization* y activaciones *Swish*.

2. **Global Average Pooling (GAP)**  
   - Reduce el volumen tridimensional de salida de la EfficientNet a un vector 1D, resumiendo la informaci贸n espacial de cada mapa de activaci贸n.  
   - Evita el uso de capas Flatten, mejorando la eficiencia y reduciendo el sobreajuste.

3. **Capas Densas Personalizadas**
   - Se a帽aden varias capas **Dense** (totalmente conectadas) con activaci贸n ReLU.  
   - Estas capas permiten combinar las caracter铆sticas extra铆das por la base convolucional y generar representaciones m谩s espec铆ficas para la tarea de clasificaci贸n.

4. **Batch Normalization**
   - Normaliza las activaciones de cada capa, estabilizando el entrenamiento y acelerando la convergencia.  
   - Ayuda a reducir la sensibilidad a los cambios en la distribuci贸n de entrada.

5. **Dropout**
   - Se utiliza tras las capas densas para evitar el sobreajuste.  
   - Durante el entrenamiento, desactiva aleatoriamente una fracci贸n de neuronas (por ejemplo, `rate=0.5`), lo que obliga al modelo a no depender de conexiones espec铆ficas.  
   - En inferencia, todas las neuronas est谩n activas, pero sus salidas se escalan proporcionalmente a la tasa de Dropout.  
   - Resultado: un modelo m谩s robusto y generalizable.

6. **Capa de salida (Softmax)**
   - N煤mero de neuronas igual al n煤mero de clases (en este caso, 4).  
   - Funci贸n de activaci贸n **Softmax**, que convierte las salidas en probabilidades normalizadas para cada clase.

---

##  Entrenamiento por Etapas

El modelo se entrena en **30 etapas** para lograr una convergencia estable:

1. **Primera etapa:**  
   - Se entrena 煤nicamente la parte superior de la red (capas densas personalizadas).  
   - La base EfficientNetB1 permanece congelada.

2. **Etapas posteriores:**  
   - Se descongelan progresivamente capas de la EfficientNetB1 para realizar *fine-tuning*.  
   - Se aplican tasas de aprendizaje m谩s peque帽as para no alterar los pesos preentrenados de forma dr谩stica.

3. **Optimizador y funciones de p茅rdida:**  
   - Optimizador: **Adam** o **AdamW**.  
   - Funci贸n de p茅rdida: **Categorical Crossentropy**.  
   - M茅tricas de evaluaci贸n: *Accuracy, Recall, Precision, F1-score* y *Especificidad*.

---

##  Evaluaci贸n del Modelo

Durante el entrenamiento y validaci贸n se monitorizan las siguientes m茅tricas:

- **Accuracy (exactitud global)**  
- **Precision y Recall por clase**  
- **F1-score promedio**  
- **Especificidad (minimiza falsos positivos)**  

Adem谩s, se generan las siguientes visualizaciones:
- Curvas de p茅rdida y precisi贸n por 茅poca.  
- Matriz de confusi贸n.  
- Curva ROC y 谩rea bajo la curva (AUC).

---

## П Estructura del Repositorio


